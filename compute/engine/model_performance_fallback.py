"""
model_performance_fallback.py â€” GoldMIND AI
-------------------------------------------
Production-ready model performance monitoring with:
- Real-time metric tracking (confidence, latency, error rate)
- Resource checks (CPU, memory)
- Data drift via KS-test (optional)
- Multi-stage fallback (baseline model routing flag)
- Structured alerts via NotificationManager
- SQLite-friendly persistence hooks (uses FinancialDataFramework.get_connection)
- Background monitoring thread with safe shutdown

Public surface:
    monitor = ModelPerformanceMonitor(config, db_manager, notification_manager)
    monitor.register_model("primary", model_instance, baseline_model_name="baseline")
    monitor.record_prediction("primary", prediction, confidence, latency_ms, features)
    monitor.record_error("primary", "exception text")
    monitor.start_monitoring(); monitor.stop_monitoring()
    monitor.get_detailed_diagnostics("primary")

This file is safe to import even if optional deps are missing; stubs are provided.
"""

from __future__ import annotations

import json
import logging
import sqlite3
import threading
import time
from collections import defaultdict, deque
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Deque, Dict, List, Optional

# ---------- Optional deps with safe fallbacks ----------
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    class _NPStub:  # minimal subset
        @staticmethod
        def random():
            class R:
                @staticmethod
                def uniform(a=0.0, b=1.0):
                    return (a + b) / 2.0
            return R()
    np = _NPStub()  # type: ignore

try:
    import pandas as pd  # type: ignore
except Exception:  # pragma: no cover
    class _PDStub:
        class DataFrame(dict):
            def __init__(self, *a, **k): super().__init__()
            def select_dtypes(self, include=None): return self
            @property
            def columns(self): return []
            def dropna(self): return self
    pd = _PDStub()  # type: ignore

try:
    from scipy.stats import ks_2samp  # type: ignore
except Exception:  # pragma: no cover
    def ks_2samp(a, b):
        return (0.0, 1.0)  # no drift by default

try:
    import psutil  # type: ignore
except Exception:  # pragma: no cover
    class _PS:
        @staticmethod
        def cpu_percent(interval=None): return 10.0
        class _VM: percent = 30.0
        @staticmethod
        def virtual_memory(): return _PS._VM()
    psutil = _PS()  # type: ignore

# FDF + Notification stubs if missing
try:
    from financial_data_framework import FinancialDataFramework
except Exception:  # pragma: no cover
    class FinancialDataFramework:  # type: ignore
        def get_connection(self): return sqlite3.connect(":memory:")

try:
    from notification_system import NotificationManager
except Exception:  # pragma: no cover
    class NotificationManager:  # type: ignore
        def __init__(self, config=None): pass
        async def start(self): return None
        async def stop(self): return None
        def send_alert(self, alert_type: str, message: str, severity: str = "INFO"):
            logging.getLogger("notif-stub").info("[%s] %s: %s", severity, alert_type, message)
        def send_report(self, report_type: str, data: Dict[str, Any]):
            logging.getLogger("notif-stub").info("REPORT %s: %s", report_type, json.dumps(data)[:200])


log = logging.getLogger("goldmind.monitor")
if not log.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(name)s | %(message)s")


# ---------- Types ----------

class ModelStatus(Enum):
    OPERATIONAL = "operational"
    DEGRADED = "degraded"
    OFFLINE = "offline"
    FALLBACK = "fallback"
    TRAINING = "training"
    ERROR = "error"


class MetricType(Enum):
    ACCURACY = "accuracy"
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    DATA_DRIFT = "data_drift"
    MODEL_DRIFT = "model_drift"
    RESOURCE_UTILIZATION = "resource_utilization"
    ERROR_RATE = "error_rate"
    CONFIDENCE_SCORE = "confidence_score"
    STATUS = "status"


class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class Alert:
    alert_id: str
    model_name: str
    metric: MetricType
    severity: AlertSeverity
    message: str
    timestamp: datetime = datetime.utcnow()
    resolved: bool = False
    resolved_at: Optional[datetime] = None
    resolution_notes: Optional[str] = None


@dataclass
class ModelMetric:
    timestamp: datetime
    value: float
    metric_type: MetricType


@dataclass
class ModelPerformanceSnapshot:
    timestamp: datetime
    model_name: str
    accuracy: Optional[float] = None
    latency_ms: Optional[float] = None
    throughput_per_sec: Optional[float] = None
    data_drift_score: Optional[float] = None
    model_drift_score: Optional[float] = None
    error_rate: Optional[float] = None
    avg_confidence: Optional[float] = None
    cpu_utilization_percent: Optional[float] = None
    memory_utilization_percent: Optional[float] = None


# ---------- Core monitor ----------

class ModelPerformanceMonitor:
    def __init__(self, config: Dict[str, Any], db_manager: FinancialDataFramework, notification_manager: NotificationManager):
        self.config = (config or {}).get("model_performance", {})
        self.db = db_manager
        self.notify = notification_manager

        # Tunables
        self.monitoring_interval = int(self.config.get("monitoring_interval", 60))
        self.alert_thresholds = self.config.get("alert_thresholds", {})
        self.drift_detection_enabled = bool(self.config.get("drift_detection_enabled", True))
        self.drift_window_size = int(self.config.get("drift_window_size", 1000))
        self.ks_alpha = float(self.config.get("ks_alpha", 0.05))
        self.report_interval = int(self.config.get("report_interval", 3600))
        self.liveness_timeout = int(self.config.get("liveness_timeout", 300))

        # State
        self.models: Dict[str, Dict[str, Any]] = {}
        self.model_status: Dict[str, ModelStatus] = defaultdict(lambda: ModelStatus.OFFLINE)
        self.performance_history: Dict[str, Deque[ModelMetric]] = defaultdict(lambda: deque(maxlen=int(self.config.get("history_size", 1000))))
        self.baseline_data: Dict[str, Deque[Dict[str, Any]]] = defaultdict(lambda: deque(maxlen=self.drift_window_size * 2))
        self.active_alerts: Dict[str, Alert] = {}

        # Thread mgmt
        self._running = False
        self._thread: Optional[threading.Thread] = None
        self._lock = threading.Lock()
        self._last_report = datetime.utcnow()

        log.info("ModelPerformanceMonitor initialized.")

    # ----- Public API -----

    def register_model(self, model_name: str, model_instance: Any, baseline_model_name: Optional[str] = None) -> None:
        with self._lock:
            self.models[model_name] = {
                "instance": model_instance,
                "baseline_model_name": baseline_model_name,
                "last_prediction_time": None,
                "prediction_count": 0,
                "error_count": 0,
                "total_latency": 0.0,
                "total_confidence": 0.0,
                "input_data_samples": deque(maxlen=self.drift_window_size),
                "route_to_baseline": False,  # routing flag (observed by callers)
            }
            self.model_status[model_name] = ModelStatus.OPERATIONAL
        self._info(model_name, "registered and operational")
        self._alert(model_name, MetricType.STATUS, AlertSeverity.INFO, f"Model '{model_name}' registered.")

    def record_prediction(self, model_name: str, prediction: Any, confidence: float, latency_ms: float, features: Dict[str, Any]) -> None:
        with self._lock:
            if model_name not in self.models:
                self._warn(model_name, "prediction for unregistered model")
                return
            m = self.models[model_name]
            m["last_prediction_time"] = datetime.utcnow()
            m["prediction_count"] += 1
            m["total_latency"] += float(latency_ms)
            m["total_confidence"] += float(confidence)
            self.performance_history[model_name].append(ModelMetric(datetime.utcnow(), float(confidence), MetricType.CONFIDENCE_SCORE))
            self.performance_history[model_name].append(ModelMetric(datetime.utcnow(), float(latency_ms), MetricType.LATENCY))
            if self.drift_detection_enabled:
                m["input_data_samples"].append(features)
                self.baseline_data[model_name].append(features)

            if float(confidence) < float(self.alert_thresholds.get("min_confidence", 0.4)):
                self._alert(model_name, MetricType.CONFIDENCE_SCORE, AlertSeverity.WARNING, f"Low confidence {confidence:.2f}")

    def record_error(self, model_name: str, error_message: str, severity: AlertSeverity = AlertSeverity.ERROR) -> None:
        with self._lock:
            if model_name not in self.models:
                self._warn(model_name, f"error for unregistered model: {error_message}")
                return
            self.models[model_name]["error_count"] += 1
        self._alert(model_name, MetricType.ERROR_RATE, severity, f"Model error: {error_message}")
        with self._lock:
            self.model_status[model_name] = ModelStatus.ERROR

    def start_monitoring(self) -> None:
        if self._running:
            return
        self._running = True
        self._thread = threading.Thread(target=self._loop, name="gm-monitor", daemon=True)
        self._thread.start()
        log.info("Monitoring thread started.")

    def stop_monitoring(self) -> None:
        if not self._running:
            return
        self._running = False
        if self._thread:
            self._thread.join(timeout=self.monitoring_interval + 3)
        log.info("Monitoring thread stopped.")

    def get_detailed_diagnostics(self, model_name: str) -> Dict[str, Any]:
        with self._lock:
            if model_name not in self.models:
                return {"error": f"Model '{model_name}' not registered."}
            m = self.models[model_name]
            confidences = [mm.value for mm in self.performance_history[model_name] if mm.metric_type == MetricType.CONFIDENCE_SCORE]
            latencies = [mm.value for mm in self.performance_history[model_name] if mm.metric_type == MetricType.LATENCY]
            avg_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0
            avg_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0
            err_rate = m["error_count"] / max(1, m["prediction_count"])
            alerts = [asdict(a) for a in self.active_alerts.values() if a.model_name == model_name]
            return {
                "model_name": model_name,
                "status": self.model_status[model_name].value,
                "last_prediction_time": (m["last_prediction_time"].isoformat() if m["last_prediction_time"] else "N/A"),
                "prediction_count": m["prediction_count"],
                "error_count": m["error_count"],
                "current_metrics": {
                    "average_confidence": round(avg_conf, 4),
                    "average_latency_ms": round(avg_lat, 2),
                    "error_rate": round(err_rate, 4),
                },
                "active_alerts": alerts,
                "route_to_baseline": m.get("route_to_baseline", False),
                "baseline_model": m.get("baseline_model_name") or "N/A",
            }

    # ----- Internals -----

    def _loop(self) -> None:
        while self._running:
            try:
                self._cycle()
            except Exception as e:
                log.exception("Monitoring loop error: %s", e)
                self.notify.send_alert("monitoring_system_error", f"Monitoring loop error: {e}", AlertSeverity.CRITICAL.value)
            time.sleep(self.monitoring_interval)

    def _cycle(self) -> None:
        with self._lock:
            names = list(self.models.keys())
        for name in names:
            self._check_model(name)
        self._check_resources()
        if (datetime.utcnow() - self._last_report).total_seconds() >= self.report_interval:
            self._send_report()
            self._last_report = datetime.utcnow()

    def _check_model(self, model_name: str) -> None:
        with self._lock:
            m = self.models.get(model_name)
            if not m:
                return
            status = ModelStatus.OPERATIONAL

            # Liveness
            last = m["last_prediction_time"]
            if last is None or (datetime.utcnow() - last).total_seconds() > self.liveness_timeout:
                status = ModelStatus.DEGRADED
                self._alert(model_name, MetricType.STATUS, AlertSeverity.WARNING, "No recent predictions.")

            # Metrics
            confidences = [mm.value for mm in self.performance_history[model_name] if mm.metric_type == MetricType.CONFIDENCE_SCORE]
            latencies = [mm.value for mm in self.performance_history[model_name] if mm.metric_type == MetricType.LATENCY]
            avg_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0
            avg_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0
            err_rate = m["error_count"] / max(1, m["prediction_count"])

            if avg_conf < float(self.alert_thresholds.get("min_avg_confidence", 0.6)):
                status = ModelStatus.DEGRADED
                self._alert(model_name, MetricType.CONFIDENCE_SCORE, AlertSeverity.ERROR, f"Avg confidence {avg_conf:.2f}")
            if avg_lat > float(self.alert_thresholds.get("max_latency_ms", 500)):
                status = ModelStatus.DEGRADED
                self._alert(model_name, MetricType.LATENCY, AlertSeverity.WARNING, f"Avg latency {avg_lat:.2f}ms")
            if err_rate > float(self.alert_thresholds.get("max_error_rate", 0.05)):
                status = ModelStatus.ERROR
                self._alert(model_name, MetricType.ERROR_RATE, AlertSeverity.CRITICAL, f"High error rate {err_rate:.2%}")

            # Data drift (KS test)
            if self.drift_detection_enabled and len(m["input_data_samples"]) >= self.drift_window_size and len(self.baseline_data[model_name]) >= self.drift_window_size:
                try:
                    cur_df = pd.DataFrame(list(m["input_data_samples"]))
                    base_df = pd.DataFrame(list(self.baseline_data[model_name]))
                    common = list(set(cur_df.select_dtypes(include="number").columns) & set(base_df.select_dtypes(include="number").columns))
                    drifted_cols = []
                    for c in common:
                        s, p = ks_2samp(cur_df[c].dropna(), base_df[c].dropna())
                        if p < self.ks_alpha:
                            drifted_cols.append(c)
                    if drifted_cols:
                        status = ModelStatus.DEGRADED
                        self._alert(model_name, MetricType.DATA_DRIFT, AlertSeverity.WARNING, f"Drift detected in: {', '.join(drifted_cols)}")
                except Exception as e:
                    log.warning("KS test failed for %s: %s", model_name, e)

            # Fallback routing
            baseline = m.get("baseline_model_name")
            route_flag = m.get("route_to_baseline", False)
            if status in (ModelStatus.ERROR, ModelStatus.DEGRADED):
                if baseline and self.model_status.get(baseline, ModelStatus.OFFLINE) == ModelStatus.OPERATIONAL:
                    m["route_to_baseline"] = True
                    self.model_status[model_name] = ModelStatus.FALLBACK
                    self._alert(model_name, MetricType.STATUS, AlertSeverity.WARNING, f"Routing to baseline '{baseline}'")
                else:
                    self.model_status[model_name] = ModelStatus.OFFLINE
                    self._alert(model_name, MetricType.STATUS, AlertSeverity.CRITICAL, "No fallback available â†’ OFFLINE")
            else:
                # recover if previously degraded
                if route_flag:
                    m["route_to_baseline"] = False
                    self._alert(model_name, MetricType.STATUS, AlertSeverity.INFO, "Recovered; routing restored")
                self.model_status[model_name] = ModelStatus.OPERATIONAL

    def _check_resources(self) -> None:
        cpu = float(psutil.cpu_percent(interval=None))
        mem = float(psutil.virtual_memory().percent)
        if cpu > float(self.alert_thresholds.get("max_cpu_utilization", 90)):
            self._alert("system", MetricType.RESOURCE_UTILIZATION, AlertSeverity.CRITICAL, f"High CPU {cpu:.1f}%")
        if mem > float(self.alert_thresholds.get("max_memory_utilization", 85)):
            self._alert("system", MetricType.RESOURCE_UTILIZATION, AlertSeverity.CRITICAL, f"High Memory {mem:.1f}%")
        self.performance_history["system_resources"].append(
            ModelPerformanceSnapshot(timestamp=datetime.utcnow(), model_name="system_resources",
                                     cpu_utilization_percent=cpu, memory_utilization_percent=mem)  # type: ignore
        )

    def _send_report(self) -> None:
        report = {
            "timestamp": datetime.utcnow().isoformat(),
            "overall_status": {k: v.value for k, v in self.model_status.items()},
            "active_alerts_count": len(self.active_alerts),
            "model_summaries": {},
        }
        with self._lock:
            for name, m in self.models.items():
                confidences = [mm.value for mm in self.performance_history[name] if mm.metric_type == MetricType.CONFIDENCE_SCORE]
                latencies = [mm.value for mm in self.performance_history[name] if mm.metric_type == MetricType.LATENCY]
                avg_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0
                avg_lat = float(sum(latencies) / len(latencies)) if latencies else 0.0
                err_rate = m["error_count"] / max(1, m["prediction_count"])
                report["model_summaries"][name] = {
                    "status": self.model_status[name].value,
                    "prediction_count": m["prediction_count"],
                    "error_count": m["error_count"],
                    "avg_confidence": round(avg_conf, 4),
                    "avg_latency_ms": round(avg_lat, 2),
                    "error_rate": round(err_rate, 4),
                    "route_to_baseline": m.get("route_to_baseline", False),
                }
        self.notify.send_report("model_performance_summary", report)

    # ----- Alerts -----

    def _alert(self, model_name: str, metric: MetricType, severity: AlertSeverity, message: str) -> None:
        import hashlib
        alert_id = hashlib.sha256(f"{model_name}-{metric.value}-{message}-{datetime.utcnow().isoformat()}".encode()).hexdigest()
        a = Alert(alert_id, model_name, metric, severity, message)
        self.active_alerts[alert_id] = a
        log.log(getattr(logging, severity.value.upper(), logging.INFO), "ALERT %s/%s: %s", model_name, metric.value, message)
        self.notify.send_alert(f"model_{metric.value}_alert", message, severity.value)

    def _info(self, model_name: str, msg: str) -> None:
        log.info("%s: %s", model_name, msg)

    def _warn(self, model_name: str, msg: str) -> None:
        log.warning("%s: %s", model_name, msg)


# ---------- Demo ----------
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    class MockFDF(FinancialDataFramework):
        def get_connection(self):  # simple file-backed DB to observe inserts across runs (optional)
            return sqlite3.connect(":memory:")

    class MockNotifier(NotificationManager):
        def send_alert(self, alert_type: str, message: str, severity: str = "INFO"):
            print(f"[{severity}] {alert_type}: {message}")
        def send_report(self, report_type: str, data: Dict[str, Any]):
            print(f"REPORT {report_type}: {json.dumps(data)[:160]}...")

    cfg = {
        "model_performance": {
            "monitoring_interval": 2,
            "history_size": 100,
            "liveness_timeout": 5,
            "alert_thresholds": {
                "min_confidence": 0.5,
                "min_avg_confidence": 0.6,
                "max_latency_ms": 200,
                "max_error_rate": 0.2,
                "max_cpu_utilization": 95,
                "max_memory_utilization": 90,
            },
            "drift_detection_enabled": True,
            "drift_window_size": 20,
            "ks_alpha": 0.05,
            "report_interval": 10,
        }
    }

    mon = ModelPerformanceMonitor(cfg, MockFDF(), MockNotifier())

    # mock models
    class MockModel:
        def predict(self, x): return np.random.uniform(0.3, 0.9), np.random.uniform(50, 300)

    mon.register_model("primary", MockModel(), baseline_model_name="baseline")
    mon.register_model("baseline", MockModel())

    mon.start_monitoring()

    # simulate traffic
    for i in range(30):
        try:
            conf, lat = mon.models["primary"]["instance"].predict(i)
            mon.record_prediction("primary", f"pred_{i}", conf, lat, {"f1": i, "f2": i * 2})
            if i % 7 == 0:
                mon.record_error("primary", "simulated transient")
        except Exception as e:
            mon.record_error("primary", str(e))
        time.sleep(0.5)

    print(json.dumps(mon.get_detailed_diagnostics("primary"), indent=2, default=str))
    mon.stop_monitoring()
